import numpy as np
import pandas as pd
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_auc_score
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import RocCurveDisplay
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis




def drop_correlated_features(df, threshold):
    '''
    Define a function that drops features that are highly correlated with each other
    The threshold is the correlation value above which a feature is considered highly correlated.
    Return a list of the columns names that should be dropped
    '''
    #Generated by ChatGPT
    corr_matrix = df.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
    return to_drop

def add_polynomical_features(df,degree):
    '''
    Define a function that adds polynomial features to the dataframe
    For each feature in the dataframe, adding n-1 features that are the power of 2 to n of the original feature
    (n is the number of degrees)
    '''
    for column in df.columns:
        if column in ['Cr', 'Co', 'Ti', 'Zr']:
                for i in range(2,degree+1):
                    df[column+'^'+str(i)] = df[column]**i
    return df

def group_based_on_ternary(df):
    '''
    Add a column to the dataframe that indicates the group of the ternary system
    :param df:
    :return: df
    '''
    group = np.zeros(len(df))
    for i in range(len(df)):   #seperate each ternary system into three different groups
        if df['Cr'][i] == 0:
            group[i] = 0
        elif df['Co'][i] == 0:
            group[i] = 1
        elif df['Ti'][i] == 0:
            group[i] = 2
    df['group'] = group

    return df

def LOGO(mat_X,mat_y,ternary_group):

    logo = LeaveOneGroupOut()
    logo.get_n_splits(mat_X, mat_y, groups=ternary_group)

    loco_list=[] #each group has: train_index, test_index, X_train, X_test, y_train, y_test
    #logo.split(mat_X, mat_y, groups=ternary_group)

    i=0
    for train_index, test_index in logo.split(mat_X, mat_y, ternary_group):
            print("TRAIN:", len(train_index), "TEST:", len(test_index))
            mat_X_train, mat_X_test = mat_X.loc[train_index], mat_X.loc[test_index]
            mat_y_train, mat_y_test = mat_y.loc[train_index], mat_y.loc[test_index]

            #standarizing the data
            sc = StandardScaler()
            sc.fit(mat_X_train)
            mat_X_train = sc.transform(mat_X_train)
            mat_X_test = sc.transform(mat_X_test)

            loco_list.append([train_index, test_index, mat_X_train, mat_X_test, mat_y_train, mat_y_test])

    return loco_list


def random_cv(mat_X,mat_y,n_split=5):
    #KNN = KNeighborsClassifier(n_neighbors=5)
    kf = KFold(n_splits=n_split, shuffle=True)
    #mat_X = mat_X.to_numpy()
    #mat_y = mat_y.to_numpy()

    random_list = []
    for train_index, test_index in kf.split(mat_X):
        X_train, X_test = mat_X.loc[train_index], mat_X.loc[test_index]
        y_train, y_test = mat_y.loc[train_index], mat_y.loc[test_index]

        sc = StandardScaler()
        sc.fit(X_train)
        mat_X_train = sc.transform(X_train)
        mat_X_test = sc.transform(X_test)

        # KNN.fit(X_train, y_train)
        # y_pred = KNN.predict(X_test)

        random_list.append([train_index, test_index, X_train, X_test, y_train, y_test])

    return random_list


def KNN_cv(cv_list, n_split=5, plot=True):

    KNN = KNeighborsClassifier(n_neighbors=n_split)

    fig, axs = plt.subplots(nrows=len(cv_list), ncols=2, figsize=(10, 5*len(cv_list)))
    for i, group in enumerate(cv_list):
        train_index, test_index, X_train, X_test, y_train, y_test = group
        KNN.fit(X_train,y_train)
        y_test_pred = KNN.predict(X_test)

        if plot:
            ConfusionMatrixDisplay.from_estimator(KNN, X_test, y_test, ax=axs[i][0])

            AUC = roc_auc_score(y_test, KNN.predict_proba(X_test)[:, 1])
            RocCurveDisplay.from_estimator(KNN,X_test,y_test, ax=axs[i][1])
            axs[i][1].set_title('Fold {}: AUC = {:.2f}'.format(i+1, AUC))

    fig.tight_layout()


def LDA_cv(cv_list, plot=True):

    LDA = LinearDiscriminantAnalysis()

    fig, axs = plt.subplots(nrows=len(cv_list), ncols=2, figsize=(10, 5*len(cv_list)))
    for i, group in enumerate(cv_list):
        train_index, test_index, X_train, X_test, y_train, y_test = group
        LDA.fit(X_train,y_train)
        y_test_pred = LDA.predict(X_test)

        if plot:
            ConfusionMatrixDisplay.from_estimator(LDA, X_test, y_test, ax=axs[i][0])

            AUC = roc_auc_score(y_test, LDA.predict_proba(X_test)[:, 1])
            RocCurveDisplay.from_estimator(LDA,X_test,y_test, ax=axs[i][1])
            axs[i][1].set_title('Fold {}: AUC = {:.2f}'.format(i+1, AUC))

    fig.tight_layout()


def KNN_entire_model(mat_X, mat_y, n_split=5):
    KNN = KNeighborsClassifier(n_neighbors=n_split)

    sc = StandardScaler()
    sc.fit(mat_X)
    mat_X = sc.transform(mat_X)

    KNN.fit(mat_X, mat_y)
    y_pred = KNN.predict(mat_X)

    fig, axs = plt.subplots(1, 2, figsize=(10, 5))

    ConfusionMatrixDisplay.from_estimator(KNN, mat_X, mat_y, ax=axs[0])

    AUC = roc_auc_score(mat_y, KNN.predict_proba(mat_X)[:, 1])
    RocCurveDisplay.from_estimator(KNN,mat_X,mat_y, ax=axs[1])
    #axs[i][1].set_title('Fold {}: AUC = {:.2f}'.format(1, AUC))

    fig.tight_layout()


